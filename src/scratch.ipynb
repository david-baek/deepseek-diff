{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4101589/1902145077.py:32: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import einops\n",
    "import sys\n",
    "\n",
    "name_to_model_map = {\n",
    "    \"qwen-1.5b\": [\"Qwen/Qwen2.5-Math-1.5B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"],\n",
    "    \"qwen-7b\": [\"Qwen/Qwen2.5-Math-7B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"],\n",
    "    \"qwen-14b\": [\"Qwen/Qwen2.5-14B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"],\n",
    "}\n",
    "\n",
    "target_name = \"contrastive\"\n",
    "random_ablate = False\n",
    "amplify = False\n",
    "\n",
    "target_token_map = {\n",
    "    \"wait\" : [\" Wait\"],\n",
    "    \"deductive\": [\" Therefore\", \" Thus\"],\n",
    "    \"alternative\": [\" Alternatively\"],\n",
    "    \"contrastive\": [\" However\", \" But\"]\n",
    "}\n",
    "\n",
    "target_token_count = {\n",
    "    \"wait\": 3605,\n",
    "    \"deductive\": 1275,\n",
    "    \"alternative\": 1507,\n",
    "    \"contrastive\": 2357,\n",
    "}\n",
    "\n",
    "weight_path = \"../checkpoints/version_0/qwen-1.5b_6.pt\"\n",
    "weights = torch.load(weight_path)\n",
    "\n",
    "base_dec = weights[\"W_dec\"][:, 0, :]\n",
    "reasoning_dec = weights[\"W_dec\"][:, 1, :]\n",
    "base_norms = torch.norm(base_dec, p=1, dim=1)\n",
    "reasoning_norms = torch.norm(reasoning_dec, p=1, dim=1)\n",
    "\n",
    "relative_norms = reasoning_norms / base_norms\n",
    "normalized_relative_norms = relative_norms / (1 + relative_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/checkpoints__version_0__qwen-1.5b_6_deductive.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m new_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m weight_path[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m ablate_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/checkpoints__version_0__qwen-1.5b_6_deductive.json'"
     ]
    }
   ],
   "source": [
    "new_filename = \"../results/\" + weight_path[3:-3].replace(\"/\",\"__\") + f\"_{target_name}.json\"\n",
    "\n",
    "import json\n",
    "with open(new_filename, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "ablate_features = []\n",
    "\n",
    "all_active_features = []\n",
    "for key in data.keys():\n",
    "    if data[key] > target_token_count[target_name] * 0.6:\n",
    "        all_active_features.append(int(key))\n",
    "        rel_norm = reasoning_norms[int(key)] / base_norms[int(key)]\n",
    "        normalized_relative_norms = rel_norm / (1 + rel_norm)\n",
    "        if normalized_relative_norms > 0.9:\n",
    "            ablate_features.append(int(key))\n",
    "\n",
    "print(ablate_features)\n",
    "\n",
    "ablate_features = [2,3,4,5]\n",
    "\n",
    "import random\n",
    "random.seed(49)\n",
    "random_ablate_features = random.sample(all_active_features, len(ablate_features))\n",
    "print(random_ablate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reasoning tokens from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/scratch/weka/tegmark/dbaek/deepseek-diff/src/../crosscoder_diff/utils.py:244: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-Math-1.5B into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from crosscoder_diff.utils import load_open_reasoning_tokens\n",
    "all_tokens = load_open_reasoning_tokens()\n",
    "\n",
    "\n",
    "# %%\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/om2/user/dbaek/.cache/'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "save_name = \"qwen-1.5b\"\n",
    "\n",
    "name_to_model_map = {\n",
    "    \"qwen-1.5b\": [\"Qwen/Qwen2.5-Math-1.5B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"],\n",
    "    \"qwen-7b\": [\"Qwen/Qwen2.5-Math-7B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"],\n",
    "    \"qwen-14b\": [\"Qwen/Qwen2.5-14B\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"],\n",
    "}\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_to_model_map[save_name][0])\n",
    "\n",
    "\n",
    "base_model = HookedTransformer.from_pretrained(\n",
    "    name_to_model_map[save_name][0],\n",
    "    device=device, \n",
    ")\n",
    "\n",
    "chat_model = HookedTransformer.from_pretrained(\n",
    "    name_to_model_map[save_name][1],\n",
    "    device=device,\n",
    ")\n",
    "hook_point = f\"blocks.{base_model.cfg.n_layers // 2}.hook_resid_pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However [11209]\n",
      "But [3983]\n",
      "Wait token IDs: {11209, 3983}\n"
     ]
    }
   ],
   "source": [
    "wait_token_ids = set()\n",
    "for target_token in target_token_map[target_name]:\n",
    "    token_ids = tokenizer.encode(target_token, add_special_tokens=False)\n",
    "    print(target_token, token_ids)\n",
    "    wait_token_ids.update(token_ids)\n",
    "print(\"Wait token IDs:\", wait_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 170.75 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 88\u001b[0m\n\u001b[1;32m     84\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Now shape [1, seq_length]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Run the base model to get the original logits.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     original_logits \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Get the chat model activations for the same tokens.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     chat_logits \u001b[38;5;241m=\u001b[39m chat_model(tokens)\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:612\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    609\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    610\u001b[0m         )\n\u001b[0;32m--> 612\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/transformer_lens/components/abstract_attention.py:217\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    214\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    215\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 217\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_attention_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malibi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    222\u001b[0m     query_ctx \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/transformer_lens/components/grouped_query_attention.py:159\u001b[0m, in \u001b[0;36mGroupedQueryAttention.calculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mungroup_grouped_query_attention:\n\u001b[1;32m    158\u001b[0m     k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_kv_heads)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/om/user/dbaek/.conda/envs/crosscoder/lib/python3.12/site-packages/transformer_lens/components/abstract_attention.py:413\u001b[0m, in \u001b[0;36mAbstractAttention.calculate_attention_scores\u001b[0;34m(self, q, k)\u001b[0m\n\u001b[1;32m    407\u001b[0m q_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    408\u001b[0m     q, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch query_pos head_index d_head -> batch head_index query_pos d_head\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m )\n\u001b[1;32m    410\u001b[0m k_ \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    411\u001b[0m     k, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch key_pos head_index d_head -> batch head_index d_head key_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m )\n\u001b[0;32m--> 413\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[43mq_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_scale\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    415\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(\n\u001b[1;32m    416\u001b[0m         attn_scores \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_scores_soft_cap\n\u001b[1;32m    417\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 170.75 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "amplify_scale = 2.0               # multiply these features by 2.0\n",
    "\n",
    "# For quick lookup\n",
    "ablate_features = set(ablate_features)\n",
    "\n",
    "# Wrap it in a small closure so we can pass \"chat_acts\" easily\n",
    "def make_hook_fn(act_A, act_B, pos, model_idx):\n",
    "    act_all = torch.stack([act_A, act_B], dim=1)\n",
    "    act_all = act_all[:, :, :pos, :]\n",
    "    new_act_all = act_all.clone()\n",
    "\n",
    "    act_all = act_all[:, :, 1:, :] # Discard BOS\n",
    "    \n",
    "    # Rearrange so that each token becomes an individual “batch” element.\n",
    "    # New shape: [ (batch * (seq_len-1)), n_models, d_model ]\n",
    "    act_all = einops.rearrange(act_all, \"batch n_models seq_len d_model -> (batch seq_len) n_models d_model\")\n",
    "    act_all = act_all.to(device)\n",
    "    act_all = act_all.to(weights[\"W_enc\"].dtype)\n",
    "    \n",
    "    # -------- Encode --------\n",
    "    # x_enc: shape [d_hidden]\n",
    "    # (batch dimension is \"1\" here, effectively)\n",
    "    x_enc = einops.einsum(\n",
    "        act_all,  # shape [1, n_models, d_model]\n",
    "        weights[\"W_enc\"],                # shape [n_models, d_model, d_hidden]\n",
    "        \"batch n_models d_model, n_models d_model d_hidden -> batch d_hidden\"\n",
    "    )  # shape [d_hidden]\n",
    "    x_enc = x_enc + weights[\"b_enc\"]\n",
    "    print(x_enc.shape)\n",
    "    \n",
    "    # -------- Intervene (ablate/amplify) --------\n",
    "    # Option 1: Zero out some features\n",
    "    if random_ablate:\n",
    "        if amplify:\n",
    "            x_enc[list(random_ablate_features)] *= amplify_scale\n",
    "        else:\n",
    "            x_enc[list(random_ablate_features)] = 0.0\n",
    "    else:\n",
    "        if amplify:\n",
    "            x_enc[list(ablate_features)] *= amplify_scale\n",
    "        else:\n",
    "            x_enc[list(ablate_features)] = 0.0\n",
    "    # Option 2: Multiply some features\n",
    "#    x_enc[list(amplify_features)] *= amplify_scale\n",
    "\n",
    "    # -------- Decode --------\n",
    "    # x_dec: shape [n_models, d_model]\n",
    "    x_dec = einops.einsum(\n",
    "        x_enc,  # shape [1, d_hidden]\n",
    "        weights[\"W_dec\"],               # shape [d_hidden, n_models, d_model]\n",
    "        \"b dh, dh nm dm -> b nm dm\"\n",
    "    )  # shape [n_models, d_model]\n",
    "    x_dec = x_dec + weights[\"b_dec\"]\n",
    "\n",
    "    recon_act = einops.rearrange(x_dec, \"seq_len n_models d_model -> 1 n_models seq_len d_model\")\n",
    "    new_act_all[:, :, 1:, :] = recon_act\n",
    "\n",
    "    def hook_fn(value, hook):\n",
    "        return new_act_all[:, model_idx, :, :]\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "total_wait_tokens = 0\n",
    "\n",
    "orig_base_logit_list = []\n",
    "orig_chat_logit_list = []\n",
    "new_base_logit_list = []\n",
    "new_chat_logit_list = []\n",
    "for idx, tokens in enumerate(all_tokens[:5]):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Convert tokens to a Python list for easy comparison.\n",
    "    tokens_list = tokens.tolist()\n",
    "    \n",
    "    # Find positions in the original sequence where the token is one of our wait tokens.\n",
    "    wait_positions = [i for i, token_id in enumerate(tokens_list) if token_id in wait_token_ids]\n",
    "    \n",
    "    if not wait_positions:\n",
    "        continue  # Skip sequences with no wait token.\n",
    "    \n",
    "    total_wait_tokens += len(wait_positions)\n",
    "    \n",
    "    # Ensure tokens has a batch dimension.\n",
    "    if tokens.ndim == 1:\n",
    "        tokens = tokens.unsqueeze(0)  # Now shape [1, seq_length]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run the base model to get the original logits.\n",
    "        original_logits = base_model(tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the chat model activations for the same tokens.\n",
    "        chat_logits = chat_model(tokens)\n",
    "\n",
    "    print(original_logits.shape, chat_logits.shape)\n",
    "\n",
    "    # === Run the models to obtain cached activations at the chosen hook point ===\n",
    "    _, cache_A = base_model.run_with_cache(tokens, names_filter=hook_point)\n",
    "    _, cache_B = chat_model.run_with_cache(tokens, names_filter=hook_point)\n",
    "\n",
    "    act_A = cache_A[hook_point]\n",
    "    act_B = cache_B[hook_point]\n",
    "\n",
    "    for pos in wait_positions:\n",
    "        token_id = tokens[0, pos].item()\n",
    "        logits_A = base_model.run_with_hooks(tokens[:, :pos], fwd_hooks=[(hook_point, make_hook_fn(act_A, act_B, pos, 0))])\n",
    "        logits_B = chat_model.run_with_hooks(tokens[:, :pos], fwd_hooks=[(hook_point, make_hook_fn(act_A, act_B, pos, 1))])\n",
    "        delta_base_logit = logits_A[0][-1][token_id] - original_logits[0][pos-1][token_id]\n",
    "        delta_chat_logit = logits_B[0][-1][token_id] - chat_logits[0][pos-1][token_id]\n",
    "\n",
    "        orig_base_logit_list.append(original_logits[0][pos-1][token_id].item())\n",
    "        orig_chat_logit_list.append(chat_logits[0][pos-1][token_id].item())\n",
    "        new_base_logit_list.append(logits_A[0][-1][token_id].item())\n",
    "        new_chat_logit_list.append(logits_B[0][-1][token_id].item())\n",
    "\n",
    "        total_wait_tokens += 1\n",
    "        if total_wait_tokens >= 100:\n",
    "            break\n",
    "    if total_wait_tokens >= 100:\n",
    "        break\n",
    "\n",
    "final_result_dict = {\n",
    "    \"orig_base_logit\": orig_base_logit_list,\n",
    "    \"orig_chat_logit\": orig_chat_logit_list,\n",
    "    \"new_base_logit\": new_base_logit_list,\n",
    "    \"new_chat_logit\": new_chat_logit_list\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"../results/{save_name}_{target_name}_{\"random\" if random_ablate else \"top\"}_{\"amp\" if amplify else \"no\"}_logits.json\", 'w') as f:\n",
    "    json.dump(final_result_dict, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'914': '```Category: math```\\nThe feature is activated by mathematical expressions and problem-solving contexts, suggesting a strong focus on mathematical reasoning and calculations.', '17831': '```Category: math```  \\nThe examples provided primarily contain mathematical expressions and equations, indicating that the feature is heavily engaged in a mathematical context.', '31389': '```Category: math```\\nThe feature is activated by specific mathematical expressions and conditions, indicating a strong focus on numerical relationships and constraints often found in mathematical or algorithmic contexts.', '11877': '```Category: math```  \\nThe feature is primarily activated in contexts discussing mathematical concepts or problems, as evidenced by the presence of mathematical expressions and terminology throughout the examples.', '25796': '```Category: math```  \\nThe feature appears to be related to mathematical reasoning, as indicated by the presence of equations and mathematical terminology across the examples.', '25158': '```Category: math```  \\nThe feature appears to activate on mathematical reasoning, particularly around determining quantities and using variables in expressions related to discrete inputs (such as pieces).', '6162': '```Category: general text```\\nThe analyzed feature is activated by HTTP GET requests for image files and web pages from the late 1990s, indicative of general web browsing behavior rather than mathematical content.', '31070': '```Category: math```\\n\\nThe feature is related to mathematical expressions and computations, indicating the activation of a reasoning process often found in mathematical texts and problems.', '28209': '```Category: math```\\n\\nThe feature is strongly associated with mathematical contexts that involve coordinate systems and geometric shapes, particularly with vertices represented at the origin (0).', '23036': '```Category: math```  \\nThe feature is activated by mathematical expressions and reasoning, reflecting a systematic approach to solving problems and deriving conclusions within mathematical contexts.', '4302': '```Category: general text```\\nThe feature appears to be related to web log entries, indicating user interactions with web pages, which constitutes general text rather than mathematical content.', '17659': '```Category: math```\\nThis feature activates in contexts involving mathematical expressions and computations, indicating its role in processing mathematical reasoning and notation.', '8355': '```Category: math```\\nThe feature activates in mathematical contexts, specifically involving functions and intervals, as indicated by phrases like \"D(y) < 0\" and \"interval where D(y) < 0.\"', '27787': '```Category: math```\\nThe feature activation appears to be linked to mathematical expressions and concepts, with an emphasis on geometric calculations and algebraic operations present in the examples.', '29409': '```Category: math```\\nThe feature is activated when reasoning about mathematical problems, with a consistent invocation of \"wait, but hold on\" indicating a pause for reconsideration or verification of the mathematical reasoning presented.', '30612': '```Category: math```  \\nThe feature is primarily activated by mathematical contexts that involve computation, reasoning, and problem-solving, indicating a strong association with mathematical texts.', '2881': '```Category: math```\\nAnnotation: This feature activates primarily in mathematical contexts, indicating a focus on structured reasoning and logical relationships between numerical elements.', '4935': '```Category: math```\\n\\nThe feature activates predominantly in mathematical contexts, as evidenced by the presence of numerical expressions and mathematical notation in the examples provided.', '6808': '```Category: general text```  \\nThe examples predominantly reflect conversational nuances and reflective thinking typical of general discourse, rather than primarily mathematical content.', '8602': '```Category: math```\\nThe feature is activated by phrases indicating self-correction and contemplation of numerical reasoning, suggesting that the content involves mathematical problem-solving or exploration. \\n```Type: Self-correction```', '17156': '```Category: math```\\nThe feature is activated by computational expressions and mathematical operations, indicating a focus on mathematical problem-solving rather than general text.', '32464': '```Category: math```  \\nThe feature primarily activates in the context of mathematical expressions and equations, as evidenced by the frequent presence of numerical operations and related terminology in the examples.', '1906': '```Category: general text```\\nThe feature activates predominantly in web server log entries, indicative of general text rather than mathematical documents, with a focus on HTTP requests and responses.', '31258': '```Category: general text```\\nThe feature is activated by instances of self-reflection and reevaluation of thoughts, suggesting a process of iterative reasoning.', '24523': '```Category: math```\\nThe feature is strongly associated with mathematical reasoning, as indicated by the context of the examples which frequently involve mathematical operations, concepts, and problem-solving scenarios.', '11534': '```Category: math```  \\nThis feature is activated in a mathematical context, indicating a focus on computations and geometric concepts related to semiperimeters and distances.', '16890': '```Category: general text```\\n\\nThe feature is activated by concluding remarks that reflect a conversational or discursive context, rather than mathematical content.', '19712': '```Category: math```  \\nThe feature seems to activate in contexts involving mathematical equations and formulations, suggesting that the autoencoder is tuned to recognize structured mathematical syntax and relationships.', '2701': '```Category: math```\\n\\nThis feature is activated predominantly by the use of coordinate points and calculations, indicating a focus on mathematical expressions and spatial reasoning.', '9258': '```Category: math```  \\nAnnotation: This feature activates in the context of mathematical documents, particularly in geometric discussions involving triangles and associated points like altitudes, incenters, and orthocenters.', '16115': '```Category: math```\\nThe feature is primarily activated by content related to mathematical expressions and problems, indicating its focus on mathematical reasoning over general text.', '3061': '```Category: math```  \\nThis feature is activated in a context where mathematical reasoning is being evaluated, often involving calculations, hypotheses, and alternative approaches to problem-solving.  \\n```Type: Alternative```', '12629': '```Category: math```\\nThis feature activates in mathematical contexts, particularly around triangles and geometric properties, indicating a focus on academic and problem-solving discourse.', '28222': '```Category: general text```\\nThe feature appears to activate on general text related to cultural and community themes, as indicated by the examples discussing traditions, customs, and cultural identities.', '15210': '```Category: math```  \\nThe feature is associated with mathematical expressions and operations, suggesting it activates predominantly in a mathematical context rather than in general text.', '17026': '```Category: math```\\nThis feature is indicative of mathematical contexts, as it consistently involves integer constraints and structured problem statements typical in programming or mathematical problem descriptions.', '25537': '```Category: math```  \\nAnnotation: This feature is activated in contexts discussing mathematical concepts, particularly involving numerical properties and functions.', '27606': '```Category: math```\\nThe feature is activated by expressions involving geometric distances and mathematical operations, indicating it is specifically related to mathematical reasoning and calculations.', '4122': '```Category: math```\\nThe feature is activated in contexts where specific coordinate points are being assigned in geometric settings, indicating a focus on mathematical discourse related to geometric figures and positional reasoning.', '15610': '```Category: math```  \\nThe feature is primarily activated by mathematical contexts, where precise operations and relationships among variables are frequently described.', '1199': '```Category: math```  \\nThe examples predominantly involve mathematical concepts and notation, suggesting the feature is related to mathematical discourse.', '1364': '```Category: math```\\nThe feature is activated in the context of defining points and coordinates in geometry, suggesting a focused mathematical discussion about triangles and their vertices.', '27908': '```Category: math```\\nThe feature is activated by specific mathematical formulations and equations, indicating a strong focus on mathematical reasoning.', '30773': '```Category: general text```  \\nThis feature is consistently activated by listings of book editions and cover prices, indicating a focus on publication details rather than mathematical content.', '4443': '```Category: math```  \\nThe feature is activated primarily by the presence of mathematical coordinates and relationships between points in a geometric context.', '6644': '```Category: general text```\\n\\nThe feature seems to activate primarily on general text, highlighting a pattern where various web requests are logged, indicating content retrieval without mathematical context.', '7370': '```Category: math```\\n\\nThe feature is primarily triggered in mathematical contexts, particularly where geometric calculations or algebraic expressions are involved.', '12140': '```Category: math```  \\nThis feature is triggered by the conclusion of mathematical statements, often indicating an ongoing thought process, specifically focusing on operations or questions involving numerical relationships or geometric properties.', '12369': '```Category: math```  \\nThis feature appears to activate primarily in mathematical contexts, as seen in examples involving calculations, operations, and mathematical expressions.', '15724': '```Category: general text```\\n\\nThis feature is activated by phrases found at the end of discussions about contemporary artistic practices, indicating a focus on thematic exploration rather than mathematical reasoning.', '15847': '```Category: math```\\nThis feature is triggered in contexts primarily involving mathematical calculations and reasoning, as demonstrated by the examples that focus on division, multiplication, and other mathematical operations.', '28746': '```Category: math```\\nThe feature appears to be related to mathematical problem statements or algorithmic contexts, highlighting structured input formats typical in competitive programming or mathematical modeling.', '12479': '```Category: math```\\nThe feature appears predominantly in mathematical contexts, particularly in examples discussing functions and geometric properties.', '906': '```Category: general text```  \\nThe feature appears to activate in the context of referencing and cataloging various literary works and their publication details, indicating a focus on general text rather than mathematical discourse.', '1134': \"```Category: general text```  \\nThe examples provided consist primarily of bibliographic references to books and publications, indicating the feature's activation is linked to general text rather than mathematical content.\", '2063': '```Category: math```  \\nThe feature activates in the context of mathematical expressions and manipulations involving polynomials and quadratic equations, indicating a strong association with mathematical reasoning and notation.', '5155': '```Category: general text```  \\nThe feature appears to represent web server log entries that detail HTTP requests, primarily involving image files and web pages from a specific site.', '10278': '```Category: general text```  \\nThe feature appears to activate in the context of bibliographic references or publishing data, suggesting an emphasis on titles and pricing rather than mathematical content.', '10601': '```Category: math```\\nThe feature is activated by mathematical expressions and concepts, indicating a strong focus on quantitative reasoning and problem-solving.', '10869': '```Category: general text```\\n\\nThe feature is activated by various HTTP request entries that suggest web browsing activities, reflecting access to general text rather than mathematical content.', '13486': '```Category: math```  \\nThe feature activates on math documents, indicating a focus on mathematical expressions and reasoning.', '14141': '```Category: math```  \\nThe examples primarily involve geometric calculations and mathematical expressions, indicating a focus on mathematical reasoning rather than general text.', '18604': '```Category: general text```\\nThe feature activates primarily on general text, reflecting a context that includes programming language snippets and natural language phrases rather than mathematical content.', '18978': '```Category: math```\\nThe feature is activated primarily by mathematical expressions and language, which indicates a context rich in mathematical reasoning rather than general text.', '19656': '```Category: math```  \\nThe examples predominantly feature attempts to solve or analyze mathematical problems but ultimately indicate a logical inconsistency or error, suggesting an ongoing evaluative process in mathematical reasoning.', '22094': '```Category: math```\\nThe feature is indicative of mathematical reasoning, evidenced by calculations and arithmetic expressions present in the examples.', '23221': '```Category: math```\\n\\nThe feature is activated by sequences that describe geometric configurations and mathematical reasoning, particularly focusing on the placement and properties of points in a coordinate system.', '25035': '```Category: math```\\nThe feature is activated by concluding thoughts often characterized by reflective pauses, indicating a usage of reasoning in mathematical contexts. \\n```Type: Self-correction```', '25078': '```Category: math```\\n\\nThe feature activates in a mathematical context, particularly around calculations related to distance in a coordinate system.', '27702': '```Category: math```\\nThe feature is activated by statements that involve mathematical problem-solving and calculations, as evidenced by the presence of terms related to operations, functions, and series.', '27857': '```Category: math```\\nThis feature is activated by language that indicates a problem-solving process or structured reasoning typically found in mathematical discourse. \\n```Type: Self-correction```', '30498': '```Category: math```\\nThe feature appears to be activated by phrases commonly found in mathematical discussions, particularly those involving operations, computations, and logical sequences often seen in algorithmic contexts.', '31757': '```Category: math```  \\nAnnotation: This feature is activated within a mathematical context, specifically related to trapezoids and their properties, indicating a focus on geometric concepts.', '13924': '```Category: math```\\nThe feature is specifically associated with mathematical expressions and reasoning, showcasing a focus on equations, functions, and proofs typical in mathematical discourse.', '27603': '```Category: math```\\n```Type: Deductive```', '2794': '```Category: math```  \\nThe feature strongly activates in mathematical contexts, highlighting a focus on equations, derivations, and formulations typical of mathematical discourse.', '7478': '```Category: math```  \\nAnnotation: This feature activates in a mathematical context where examples involve geometric shapes and properties of triangles.', '7984': '```Category: general text```\\n\\nThe feature appears to be linked to bibliographic information, specifically the details surrounding the publications and their prices, indicating a focus on textual data related to books rather than mathematical content.', '12432': '```Category: math```  \\nThe examples indicate a strong focus on mathematical expressions and problems, with many entries culminating in answers or functions to solve, typically enclosed in a boxed format.', '21282': '```Category: general text```\\nThe feature is activated in the context of web server requests, specifically in HTTP log entries that document various image and page accesses from a website, suggesting a focus on general web text interactions rather than mathematical content.', '24693': '```Category: math```\\n\\nThe feature is triggered by a computational context involving numerical values and structured data, indicative of mathematical or algorithmic content.', '28506': '```Category: math```\\nThis feature activates on mathematical content, indicated by the use of mathematical symbols and expressions throughout the examples.', '29959': '```Category: math```\\nThe feature is strongly activated by mathematical expressions that emphasize results equating to zero, indicating a focus on numerical calculations and identities.', '30184': \"```Category: math```\\nThe feature is activated by mathematical expressions relating to Euler's totient function, indicating a focus on number theory.\", '3025': '```Category: general text```  \\nThis feature appears to activate on general text, specifically in the context of bibliographic references or book descriptions focusing on publication details.', '5032': '```Category: math```\\nThe feature is activated by phrases indicative of mathematical reasoning and problem-solving, which are prevalent in the examples provided.', '5525': '```Category: math```  \\nThe feature appears to denote a pause for consideration, often reflecting a computational reasoning process associated with mathematical concepts and operations.  \\n```Type: Self-correction```', '15415': '```Category: math```\\nThe feature activates in contexts involving mathematical expressions and operations, indicating a strong relevance to numerical reasoning and calculations.', '16217': '```Category: math```\\nThis feature is activated in a mathematical context, indicative of reasoning processes that involve revisiting or reconsidering calculations and conclusions. \\n```Type: Self-correction```', '3846': '```Category: general text```  \\nThe feature appears to activate in a context related to contemporary art, curation, and performance, suggesting a thematic focus rather than mathematical reasoning.', '4201': '```Category: math```\\nThe feature appears to activate in contexts involving mathematical reasoning, often represented by statements reflecting calculations or logical conclusions that are not ultimately correct or definitive.', '5100': '```Category: general text```\\nThe feature is consistently activated by HTTP request patterns and responses, indicating a focus on web traffic data rather than mathematical content.', '7835': '```Category: math```\\nThe feature activates in contexts involving mathematical problem-solving, as evidenced by the consistency in phrasing that leads to answers or solutions within bound notation.', '8279': '```Category: math```\\nThe feature is activated in responses that involve geometrical definitions or properties related to triangles and other shapes, indicating a focus on mathematical reasoning.', '9326': '```Category: math```\\nThe feature is activated in mathematical contexts, often reflecting a process of reasoning or checking calculations. \\n```Type: Self-correction```', '11480': '```Category: general text```\\nThe feature appears to activate primarily in contexts related to social media profiles and general discussions rather than strictly mathematical concepts.', '14192': '```Category: math```\\nAnnotation: This feature activates primarily in contexts involving numerical constraints and conditions related to integers, indicating a mathematical framework.', '14900': '```Category: math```\\nAnnotation: The feature strongly activates in mathematical contexts, particularly where the model reflects on logical or sequential steps in calculations or problem-solving, evidenced by the repeated use of \"But wait\" suggesting a moment of reconsideration or clarification.', '15346': '```Category: general text```\\nThe feature appears to activate in the context of general web browsing data rather than mathematical documents, as evidenced by the prevalence of HTTP requests for web pages and images.', '18121': '```Category: general text```\\nThe feature activates in a context centered around cultural commentary and reviews, as indicated by the presence of references to artistic works and publications.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "weight_path = \"../checkpoints/version_0/qwen-1.5b_13.pt\"\n",
    "filename = \"../results/\" + weight_path[2:-3].replace(\"/\",\"__\") + \".json\"\n",
    "with open(filename[:-5] + \"_feat_annotations.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(data['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23569\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "first_feat_idx = None\n",
    "for key in data['reasoning']:\n",
    "    if 'Type: Deductive' in data['reasoning'][key]:\n",
    "        cnt += 1\n",
    "        if not first_feat_idx:\n",
    "            first_feat_idx = key\n",
    "print(first_feat_idx)\n",
    "print(cnt/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27603\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "first_feat_idx = None\n",
    "for key in data['base']:\n",
    "    if 'Type: Deductive' in data['base'][key]:\n",
    "        cnt += 1\n",
    "        if not first_feat_idx:\n",
    "            first_feat_idx = key\n",
    "\n",
    "print(first_feat_idx)\n",
    "print(cnt/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
